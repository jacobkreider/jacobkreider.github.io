
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Understanding-the-Basics-of-Markov-Decision-Processes">Understanding the Basics of Markov Decision Processes<a class="anchor-link" href="#Understanding-the-Basics-of-Markov-Decision-Processes">&#182;</a></h2><p>In this post:</p>
<ul>
<li>Markov Process (or Markov Chains)</li>
<li>Markov Reward Processes</li>
<li>Markov Decision Processes</li>
</ul>
<h3 id="Markov-Process/Chain">Markov Process/Chain<a class="anchor-link" href="#Markov-Process/Chain">&#182;</a></h3><p>Let's say we are observing some type of system in a way that means we can only watch, not interact with it or change it in any way. Any change in the system is a different <em>state</em>, and all the possible states of the system is known as the <em>state space</em>. For this example, we will think about the daily change in value, positive or negative, of an imaginary stock price. We can observe the current day's state as <em>higher</em> or <em>lower</em> than the previous day's value-- this is our state space. Over time, we end up with a seuqence  of these observations, forming a <em>chain of states</em> ([higher, higher, lower, lower, higher, lower, etc]). This is the <em>history</em> of our state space.</p>
<p>In order for this sytem to be a <em>markov process</em>, it needs to satisfy the <strong>Markov property*</strong>; namely, that "future system dynamics from any state must depend on this state only" (Lapan, 12). Each state must be self-contained and not dependent on the whole history-- future states must be able to be modeled from only one state. (Of course, this is not true of stock prices in the real world, but we'll bend the rules of reality a bit for our example.)</p>
<p>When a system model fulfills the Markov property, we can build a <strong>transition matrix</strong> from the probabilities of transitioning from one state to another. Below, we can see the transition matrix for our simple stock model:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">transitionMatrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mf">0.60</span><span class="p">,</span> <span class="mf">0.40</span><span class="p">]</span>
                              <span class="p">,</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]]</span>
                              <span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Higher&quot;</span><span class="p">,</span> <span class="s2">&quot;Lower&quot;</span><span class="p">]</span>
                              <span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Higher&quot;</span><span class="p">,</span> <span class="s2">&quot;Lower&quot;</span><span class="p">])</span>
<span class="n">transitionMatrix</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[1]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Higher</th>
      <th>Lower</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Higher</th>
      <td>0.60</td>
      <td>0.40</td>
    </tr>
    <tr>
      <th>Lower</th>
      <td>0.25</td>
      <td>0.75</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we can see, if a day's stock value is higher than the day before, there is a 60% chance that the price will rise further the next day, and a 40% chance it will decline; however, if the value is lower than the previous day, there is a 75% chance it will continue to decline. Our model is pretty bearish, it seems!</p>
<h5 id="A-quick-review:">A quick review:<a class="anchor-link" href="#A-quick-review:">&#182;</a></h5><ul>
<li>A Markov process consists of a state space which contains all possible states in which  the system can be observed</li>
<li>A sequence of states forms its history, or chain of states</li>
<li>A transition matrix defines the dynamics of the system by containing the probabilities of the system transitioning between each state</li>
</ul>
<p>One more important point: the  Markov property implies <em>stationarity</em>-- the "underlying transition distribution for any state does not change over time" (Lapan, 15). If some hidden, unobserved factor changes the underlying system dynamics, then the Marvov property does not apply.</p>
<h3 id="Markov-Reward-Process">Markov Reward Process<a class="anchor-link" href="#Markov-Reward-Process">&#182;</a></h3><p>The transition matrix gives us the probability of state-to-state changes, but we need to assign values to those transitions. This will be the <em>reward</em>. Rewards can be positive or negative, of all sizes. In addition to the reward, we will also add a discount factor gamma,  $\gamma$ , which is a single number from 0 to 1. (More on this later.)</p>
<p>Since we observe a chain of states in a Markov process (and a Markov reward process), we now have a reward value for every transition in the system. With our reward values and our discount factor, we can define <strong><em>return</em></strong> as:</p>
<p>$$G_t= \sum_{i=0}^\infty \gamma^k R_{t+k+1}$$, where k = number of steps we are from our starting point at time <em>t</em>.</p>
<p>In essence, the return value is the sum of future rewards, degraded by the strength of our discount value. A discount value of 1 means we have given our agent perfect vision into future rewards, while a value of 0 means it is unable to consider anything but the current reward.</p>
<p>Gamma is going to be very important in reinforcement learning applications, but for now we will think of it as only our ability to see into the future and remember that the higher the number, the further we can see.</p>
<p>At the end of the day, individual return values don't mean too much-- they are tied to very specific chains, which means that every state can have wide variations in their return values. To make it more useful, we can take the average of a large number of possible chains for a given state, giving us a <strong><em>value of state</em></strong>.</p>
<h5 id="A-quick-review:">A quick review:<a class="anchor-link" href="#A-quick-review:">&#182;</a></h5><ul>
<li>In addition to transition probabilities, we also assign a value to each transition, called a reward</li>
<li>We can calculate a return value by multiplying the sum of future rewards by a discount factor $\gamma$</li>
<li>The larger the discount factor, the further into the future we can see</li>
<li>By averaging the returns across many chains for a given state, we get a much more useful metric called the value of state</li>
</ul>
<h3 id="Markov-Decision-Process">Markov Decision Process<a class="anchor-link" href="#Markov-Decision-Process">&#182;</a></h3><p>Next, we want to add a finite set of actions called an <strong><em>action space</em></strong> to our process. When we add action to our transition matrix from our initial Markov process, it adds an extra dimension, making a <em>transition cube</em>. Instead of passively observing state changes as we did in our stock example, we can now take an action at every single timestep. Our cube identifies the probability that state <em>i</em> will become state <em>j</em> given action <em>k</em>.</p>
<p>To make it a bit clearer, our agent can now affect the probability that the system will end up in a particular state. Not a bad ability to have!</p>
<p>Just as in the Markov Reward Process section, we are not only interested in probabilities, we also need to add these actions to our reward matrix. So, our agent doesn't just get a reward for the state the system is in (or goes into), it also gets rewarded for the actions it takes.</p>
<p>This gets us to one of the key features of both Markov Decision Processes and Reinforcement Learning-- <strong><em>policy</em></strong>. Policies are rule sets governing the behavior of our agent. Policy choice is important, because our agent will seek to maximize its return. Small changes in policy (say, rewarding a certain action more than a certain state) can have dramatic effect on the return that is achieved.</p>
<p>Policy can be defined as:</p>
<p>$$\pi(a|s) = P[A_t = a|S_t = s] $$ , or, the probability distribution over actions for every possible state.</p>
<p>One final note: If our policy stays fixed and unchanging, we can model our Markov Decision Process as a Markov Reward Process by reducing the transition and reward matrices via the policy's probabilities. No need for the action dimensions.</p>
<h5 id="A-few-notes:">A few notes:<a class="anchor-link" href="#A-few-notes:">&#182;</a></h5><ul>
<li>By adding actions with their own set of rewards, we construct a Markov Decision Process</li>
<li>The policy can be defined as the rule sets governing the behavior of our agent</li>
<li>Setting good policy is vitally important to the success of our agent</li>
</ul>
<p><em>Sources</em></p>
<p>Lapan, M. (2018). <em>Deep reinforcement learning hands-on: Apply modern RL methods, with deep Q-networks, value iteration, policy gradients, TRPO, AlphaGo Zero and more.</em></p>

</div>
</div>
</div>
 

