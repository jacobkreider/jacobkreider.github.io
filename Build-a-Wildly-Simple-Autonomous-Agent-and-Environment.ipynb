{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qiMTkr0TrZJp"
   },
   "source": [
    "<sub>*Before reading and following along with this post, it might be helpful to go read [this](https://jacobkreider.github.io/posts/understanding-the-basics-of-markov-decision-processes/) post on agents, environments, and Markov decision processes. Also, the code below is found in Maxim Laplan's excellent book, Deep Reinforcement Learning Hands-On, cited fully at the end of the post. His code without my commentary can be found [here](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter02/01_agent_anatomy.py).*</sub></br></br>\n",
    "\n",
    "## How to build a (very simple) autonomous agent and environment\n",
    "\n",
    "As a beginning exercise in my ongoing series on implementing Reinforcement Learning models, we'll define a simplsitic environment that gives rewards for a determined number of steps, regardless of the actual actions. \n",
    "\n",
    "First, a quick review:\n",
    "\n",
    "**Agent**: Some piece of code that implements some *policy*. Given observations, the policy dictates what action the agent takes at each timestep\n",
    "\n",
    "**Environment**: The model of the world, external to the agent. Provides observations, gives awards, and changes state based on actions.\n",
    "\n",
    "\n",
    "\n",
    "We'll start by initializing the internal state of the environment, which is simply a counter that limits the total number of steps the agent is allowed to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkuTsYYnsKpX"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "  def __init__(self):\n",
    "    self.steps_left = 10\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6b91BMYF4HdD"
   },
   "source": [
    "Let's now define *get_observation()*, which returns the observation of the current environment to the agent. Normally, it would be implemented as a function of the internal environment state, but in our case the vector is always zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmzcSu6a4XAS"
   },
   "outputs": [],
   "source": [
    "def get_observation(self):\n",
    "  return [0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eeX61HxPst-k"
   },
   "source": [
    "Next, we'll define *get_actions*, a set of actions from which the agent can choose. For this model, there are only two actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_i2YiuJsiOq"
   },
   "outputs": [],
   "source": [
    "def get_actions(self):\n",
    "  return [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EY45u3Aptg8h"
   },
   "source": [
    "As our agent takes actions within the environment, it performs series of steps called *episodes*. We need to define when an episode is over, so the agent knows when there is no longer any way to communicate with the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gitGqH-BstOl"
   },
   "outputs": [],
   "source": [
    "def is_done(self):\n",
    "  return self.steps_left == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ve2NWkhukrx"
   },
   "source": [
    "The *action()* method is perhaps the most important piece of the environment. The action() method both handles the agent's action (thereby changing the environment's state) and returns a reward to the agent for this action. In this case, the reward is random, and the action has no effect on the environment. Additionally, we reduce the *steps_left* counter by one. Finally, if the steps remaining == 0, then we stop the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LY4C3AcMtgEt"
   },
   "outputs": [],
   "source": [
    "def action(self, action):\n",
    "  if self.is_done():\n",
    "    raise Exception(\"Game is over\")\n",
    "  self.steps_left -= 1\n",
    "  return random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phgEH9iL1jwA"
   },
   "source": [
    "##### Defining our agent\n",
    "\n",
    "We'll begin by initializing our agent and a counter which will be used to store the reward value as it accumulates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJnXipN0ujpQ"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "  def __init__(self):\n",
    "    self.total_reward = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niuAEb9P2Yrj"
   },
   "source": [
    "Now, we'll define a *step()* function that accepts our environment as an argument, allowing the agent to do a few things:\n",
    "\n",
    "* Observe the state of the environment via *current_obs*\n",
    "* Make a decision on which action to perform via *actions*\n",
    "* Pass the action to the environment and receive the reward (*reward/env.actions*)\n",
    "* Add the current reward to the accumulated rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2mgY8n11iuk"
   },
   "outputs": [],
   "source": [
    "def step(self, env):\n",
    "  current_obs = env.get_observation() # Observe the environment\n",
    "  actions = env.get_actions() # Get available actions\n",
    "  reward = env.action(random.choice(actions)) # Perform the action, get a reward\n",
    "  self.total_reward += reward # Add reward to total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2Kpnq0W5qh5"
   },
   "source": [
    "##### A quick review:\n",
    "\n",
    "So, we've done quite a bit here, and it may not be lost on anyone following along that the above code snippets don't mean much until they are combined correctly and passed through a proedure to create the classes and run an episode. Before we do that, let's review what we've done:\n",
    "\n",
    "* We created classes for both our *environment* and our *agent*\n",
    "* We defined our *observation vector*, allowing our agent to see the environment in its current state\n",
    "* We defined the *action space* for our agent in the environment\n",
    "* We set the rules for when an episode ends\n",
    "* We defined what happens to the environment (nothing, in this case) and what rewards are given when our agent takes an actions\n",
    "* We defined what happens at each step in an episode\n",
    "\n",
    "*Whew*, a lot for a simple environment! Let's put it all together and run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "Uh3VTfUb2X60",
    "outputId": "e6850570-5ccc-4539-b8ca-f660870d767e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward collected: 4.6631\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Environment:\n",
    "  def __init__(self):\n",
    "    self.steps_left = 10\n",
    "    \n",
    "  def get_observation(self):\n",
    "    return [0.0, 0.0, 0.0]\n",
    "\n",
    "  def get_actions(self):\n",
    "    return [0, 1]\n",
    "\n",
    "  def is_done(self):\n",
    "    return self.steps_left == 0\n",
    "  \n",
    "  def action(self, action):\n",
    "    if self.is_done():\n",
    "      raise Exception(\"Game is over\")\n",
    "    self.steps_left -= 1\n",
    "    return random.random()\n",
    "  \n",
    "class Agent:\n",
    "  def __init__(self):\n",
    "    self.total_reward = 0.0\n",
    "    \n",
    "  def step(self, env):\n",
    "    current_obs = env.get_observation() # Observe the environment\n",
    "    actions = env.get_actions() # Get available actions\n",
    "    reward = env.action(random.choice(actions)) # Perform action,get reward\n",
    "    self.total_reward += reward # Add reward to total\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "  env = Environment()\n",
    "  agent = Agent()\n",
    "  \n",
    "  while not env.is_done():\n",
    "    agent.step(env)\n",
    "    \n",
    "  print(\"Total reward collected: %.4f\" % agent.total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFX44taD30bk"
   },
   "source": [
    "*Sources*\n",
    "\n",
    "Lapan, M. (2018). *Deep reinforcement learning hands-on: Apply modern RL methods, with deep Q-networks, value iteration, policy gradients, TRPO, AlphaGo Zero and more.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Build a (Wildly Simple) Autonomous Agent and Environment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
