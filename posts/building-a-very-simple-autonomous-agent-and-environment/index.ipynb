{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub>*Before reading and following along with this post, it might be helpful to go read [this](https://jacobkreider.github.io/posts/understanding-the-basics-of-markov-decision-processes/) post on agents, environments, and Markov decision processes. Also, the code below is found in Maxim Laplan's excellent book, Deep Reinforcement Learning Hands-On, cited fully at the end of the post. His code without my commentary can be found [here](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter02/01_agent_anatomy.py).*</sub></br></br>\n",
    "\n",
    "## How to build a (very simple) autonomous agent and environment\n",
    "\n",
    "As a beginning exercise in my ongoing series on implementing Reinforcement Learning models, we'll define a simplsitic environment that gives rewards for a determined number of steps, regardless of the actual actions. \n",
    "\n",
    "First, a quick review:\n",
    "\n",
    "**Agent**: Some piece of code that implements some *policy*. Given observations, the policy dictates what action the agent takes at each timestep\n",
    "\n",
    "**Environment**: The model of the world, external to the agent. Provides observations, gives awards, and changes state based on actions.\n",
    "\n",
    "##### Defining our environment\n",
    "\n",
    "We'll start by initializing the internal state of the environment, which is simply a counter that limits the total number of steps the agent is allowed to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "  def __init__(self):\n",
    "    self.steps_left = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define *get_observation()*, which returns the observation of the current environment to the agent. Normally, it would be implemented as a function of the internal environment state, but in our case the vector is always zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observation(self):\n",
    "  return [0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define *get_actions*, a set of actions from which the agent can choose. For this model, there are only two actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions(self):\n",
    "  return [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our agent takes actions within the environment, it performs series of steps called *episodes*. We need to define when an episode is over, so the agent knows when there is no longer any way to communicate with the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_done(self):\n",
    "  return self.steps_left == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *action()* method is perhaps the most important piece of the environment. The action() method both handles the agent's action (thereby changing the environment's state) and returns a reward to the agent for this action. In this case, the reward is random, and the action has no effect on the environment. Additionally, we reduce the *steps_left* counter by one. Finally, if the steps remaining == 0, then we stop the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(self, action):\n",
    "  if self.is_done():\n",
    "    raise Exception(\"Game is over\")\n",
    "  self.steps_left -= 1\n",
    "  return random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining our agent\n",
    "\n",
    "We'll begin by initializing our agent and a counter which will be used to store the reward value as it accumulates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "  def __init__(self):\n",
    "    self.total_reward = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll define a *step()* function that accepts our environment as an argument, allowing the agent to do a few things:\n",
    "\n",
    "* Observe the state of the environment via *current_obs*\n",
    "* Make a decision on which action to perform via *actions*\n",
    "* Pass the action to the environment and receive the reward (*reward/env.actions*)\n",
    "* Add the current reward to the accumulated rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, env):\n",
    "  current_obs = env.get_observation() # Observe the environment\n",
    "  actions = env.get_actions() # Get available actions\n",
    "  reward = env.action(random.choice(actions)) # Perform the action, get a reward\n",
    "  self.total_reward += reward # Add reward to total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A quick review:\n",
    "\n",
    "So, we've done quite a bit here, and it may not be lost on anyone following along that the above code snippets don't mean much until they are combined correctly and passed through a proedure to create the classes and run an episode. Before we do that, let's review what we've done:\n",
    "\n",
    "* We created classes for both our *environment* and our *agent*\n",
    "* We defined our *observation vector*, allowing our agent to see the environment in its current state\n",
    "* We defined the *action space* for our agent in the environment\n",
    "* We set the rules for when an episode ends\n",
    "* We defined what happens to the environment (nothing, in this case) and what rewards are given when our agent takes an actions\n",
    "* We defined what happens at each step in an episode\n",
    "\n",
    "*Whew*, a lot for a simple environment! Let's put it all together and run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward collected: 3.8136\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Environment:\n",
    "  def __init__(self):\n",
    "    self.steps_left = 10\n",
    "    \n",
    "  def get_observation(self):\n",
    "    return [0.0, 0.0, 0.0]\n",
    "\n",
    "  def get_actions(self):\n",
    "    return [0, 1]\n",
    "\n",
    "  def is_done(self):\n",
    "    return self.steps_left == 0\n",
    "  \n",
    "  def action(self, action):\n",
    "    if self.is_done():\n",
    "      raise Exception(\"Game is over\")\n",
    "    self.steps_left -= 1\n",
    "    return random.random()\n",
    "  \n",
    "class Agent:\n",
    "  def __init__(self):\n",
    "    self.total_reward = 0.0\n",
    "    \n",
    "  def step(self, env):\n",
    "    current_obs = env.get_observation() # Observe the environment\n",
    "    actions = env.get_actions() # Get available actions\n",
    "    reward = env.action(random.choice(actions)) # Perform action,get reward\n",
    "    self.total_reward += reward # Add reward to total\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "  env = Environment()\n",
    "  agent = Agent()\n",
    "  \n",
    "  while not env.is_done():\n",
    "    agent.step(env)\n",
    "    \n",
    "  print(\"Total reward collected: %.4f\" % agent.total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sources*\n",
    "\n",
    "Lapan, M. (2018). *Deep reinforcement learning hands-on: Apply modern RL methods, with deep Q-networks, value iteration, policy gradients, TRPO, AlphaGo Zero and more.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nikola": {
   "category": "",
   "date": "2019-04-03 20:41:59 UTC-05:00",
   "description": "",
   "link": "",
   "slug": "building-a-very-simple-autonomous-agent-and-environment",
   "tags": "",
   "title": "Building a (Very Simple) Autonomous Agent and Environment",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
